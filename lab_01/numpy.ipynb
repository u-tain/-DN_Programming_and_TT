{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\nimport os\nimport torchvision\nfrom matplotlib import pyplot as plt \nfrom sklearn. metrics import f1_score\nfrom tqdm import tqdm, tqdm_notebook\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-29T14:47:11.613752Z","iopub.execute_input":"2022-12-29T14:47:11.614283Z","iopub.status.idle":"2022-12-29T14:47:14.521406Z","shell.execute_reply.started":"2022-12-29T14:47:11.614185Z","shell.execute_reply":"2022-12-29T14:47:14.519584Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"MNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)\nMNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:14.524625Z","iopub.execute_input":"2022-12-29T14:47:14.525571Z","iopub.status.idle":"2022-12-29T14:47:17.424685Z","shell.execute_reply.started":"2022-12-29T14:47:14.525509Z","shell.execute_reply":"2022-12-29T14:47:17.422603Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9912422 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"259e883e0a4442f498aef1fbdf638a9c"}},"metadata":{}},{"name":"stdout","text":"Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28881 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd5cf2360010467aabb06b31224e2b44"}},"metadata":{}},{"name":"stdout","text":"Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1648877 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33fbdc36001a47aba4f3a5f4290b2c4b"}},"metadata":{}},{"name":"stdout","text":"Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4542 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef16d00a3e5747babd37fa349bd96c68"}},"metadata":{}},{"name":"stdout","text":"Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n\n","output_type":"stream"}]},{"cell_type":"code","source":"len(MNIST_train.train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.426905Z","iopub.execute_input":"2022-12-29T14:47:17.427448Z","iopub.status.idle":"2022-12-29T14:47:17.441493Z","shell.execute_reply.started":"2022-12-29T14:47:17.427394Z","shell.execute_reply":"2022-12-29T14:47:17.439688Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n  warnings.warn(\"train_labels has been renamed targets\")\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"60000"},"metadata":{}}]},{"cell_type":"code","source":"plt.imshow(MNIST_train.train_data[1]/255)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.444322Z","iopub.execute_input":"2022-12-29T14:47:17.445069Z","iopub.status.idle":"2022-12-29T14:47:17.625947Z","shell.execute_reply.started":"2022-12-29T14:47:17.445035Z","shell.execute_reply":"2022-12-29T14:47:17.624878Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fb5d882f250>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"class make_data():\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n        self.len = len(self.data)\n        \n    def __getitem__(self, index):\n        img = self.data[index] \n        img = np.array(img)\n        img = img.reshape((1,28,28))\n        \n        # нормализуем\n        img = img / 255\n        \n        return img, int(self.labels[index])\n    \n    def __len__(self):\n        return self.len\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.629589Z","iopub.execute_input":"2022-12-29T14:47:17.629964Z","iopub.status.idle":"2022-12-29T14:47:17.637388Z","shell.execute_reply.started":"2022-12-29T14:47:17.629930Z","shell.execute_reply":"2022-12-29T14:47:17.636193Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def custom_loader( dataset, num_batch):\n        data = []\n        amount = len(dataset) // num_batch\n        for i in range(amount):\n            batch_img = []\n            batch_tar = []\n            if i != amount:\n                for j in range(num_batch):\n                    batch_img.append(dataset[i*j][0])\n                    batch_tar.append(dataset[i*j][1])\n            else:\n                for j in range(len(dataset)-amount*num_batch):\n                    batch_img.append(dataset[i*j][0])\n                    batch_tar.append(dataset[i*j][1])\n            data.append((np.array(batch_img), np.array(batch_tar)))\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.638895Z","iopub.execute_input":"2022-12-29T14:47:17.639665Z","iopub.status.idle":"2022-12-29T14:47:17.648234Z","shell.execute_reply.started":"2022-12-29T14:47:17.639633Z","shell.execute_reply":"2022-12-29T14:47:17.646971Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"свертка","metadata":{}},{"cell_type":"code","source":"class conv2d():\n    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = None, bias = True):\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weights = np.random.rand(self.out_channels, self.in_channels, *self.kernel_size)\n        self.type = 'conv'\n        if bias:\n            self.bias = np.random.rand(out_channels)\n        else: \n            self.bias = None\n            \n            \n    # свертка    \n    def __call__(self, input_img): \n        self.input = input_img\n        if self.padding is not None:\n            input_img = self.get_padding(input_img, pad = self.padding)\n        out = self.calc_out_shape(self.input.shape, self.out_channels,self.kernel_size[0],self.stride,self.padding)\n        output = np.zeros(out) \n        for i in range(out[0]): # по каждой из картиночек\n            outs = output[i,:,:,:]\n            for j in range(input_img.shape[1]): #по каждому каналу картиночки\n                for k in range(out[2]): # по каждому пикселю высоты\n                    arr = input_img[i,j, k*self.stride:k*self.stride+self.kernel_size[0],\n                                          k*self.stride:k*self.stride+self.kernel_size[1]]\n                    for m in range(out[1]): # по каждому каналу ядра\n                        if self.bias is not None:\n                            outs[m, k, k] += np.sum(arr * self.weights[m][j]) + self.bias[m]\n                        else: \n                            outs[m, k, k] += np.sum(arr * self.weights[m][j])\n            output[i,:,:,:] = outs\n        return np.array(output)\n    \n    \n    def __str__(self):\n        return self.type\n    \n    \n    # padding\n    def get_padding(self, input, pad = 1, full = 0):\n        # img size: n_batch, n_channels, w,h\n        padd = []\n        for i in range(input.shape[0]):\n            arr = np.full((input.shape[1],input.shape[2]+2*pad,input.shape[3]+2*pad),full)\n            arr[:, pad:-pad,pad:-pad] = input[i]\n            padd.append(arr)\n        return np.array(padd)\n\n    # вычисляем выходную размерность\n    def calc_out_shape(self, input_matrix_shape, out_channels, kernel_size, stride, padding):\n        if padding == None:\n            padding = 0\n        out_shape = [input_matrix_shape[0], out_channels, (input_matrix_shape[2]+2*padding-kernel_size)//stride+1,\n                 (input_matrix_shape[3]+2*padding-kernel_size)//stride+1] \n        return np.array(out_shape)\n    \n    def get_params(self):\n        return [self.weights, self.bias]\n    \n    \n    def update_weights(self, new_params):\n        self.weight, self.bias = new_params\n    \n    \n    def backward(self, prev_grad):\n        dx = np.zeros_like(self.input) # dx = dy с 0-отступом, свернутый с «инвертированным» фильтром self.weights\n        dw = np.zeros_like(self.weights) # dw = свертка сверткой входа self.input с фильтром dy\n        db = np.zeros_like(self.bias)\n        \n        \n        db = np.sum(prev_grad, axis = (0, 2, 3))\n        \n        N, C, H, W = self.input.shape\n        F, _, HH, WW = self.weights.shape\n        _, _, H_, W_ = prev_grad.shape\n        \n        w_ = np.zeros_like(dw)\n        for i in range(HH):\n            for j in range(WW):\n                w_[:,:,i,j] = self.weights[:,:,HH-i-1,WW-j-1]\n        if self.padding is not None:\n            pad = self.padding\n            input_w_pad = self.get_padding(self.input, pad = self.padding) \n            dx_w_pad = self.get_padding(dx, pad = self.padding) \n        else:\n            pad = 0\n            input_w_pad = self.input\n            dx_w_pad = dx \n            \n        pred_grad_w_pad =  self.get_padding(prev_grad, pad = WW-1) # реализация для квадратных изображений и ядер свертки\n    \n        for n in range(N):       \n            for f in range(F):   \n                for i in range(HH): \n                    for j in range(WW):\n                        for k in range(H_): \n                            for l in range(W_):\n                                for c in range(C): \n                                    dw[f,c,i,j] += input_w_pad[n, c, self.stride*i+k, self.stride*j+l] * prev_grad[n, f, k, l]\n    \n                for i in range(H+2*pad):\n                    for j in range(W+2*pad):\n                        for k in range(HH):\n                            for l in range(WW):\n                                for c in range(C): \n                                    dx_w_pad[n,c,i,j] += pred_grad_w_pad[n, f, i+k, j+l] * w_[f, c, k, l]\n        if self.padding is not None:                            \n            dx = dx_w_pad[:,:,pad:-pad,pad:-pad]\n        else: dx = dx_w_pad\n#         print(dw)\n        return dx, dw, db        ","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.650149Z","iopub.execute_input":"2022-12-29T14:47:17.650461Z","iopub.status.idle":"2022-12-29T14:47:17.681179Z","shell.execute_reply.started":"2022-12-29T14:47:17.650433Z","shell.execute_reply":"2022-12-29T14:47:17.679886Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#maxpool\nclass maxpool2d():\n    def __init__(self,kernel_size, stride=None, padding=0, dilation=1):\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.type = 'maxpool'\n        \n    \n    def __call__(self,input):\n#         print('maxpool', input.shape)\n        self.input = input\n        if self.padding != 0:\n            input = self.get_padding(input, self.padding)\n        n = input.shape[0]\n        c = input.shape[1]\n        h = (input.shape[2]-self.kernel_size[0])//self.stride + 1\n        w = (input.shape[3]-self.kernel_size[1])//self.stride + 1\n        out_shape = [n, c, h, w]\n        output = np.zeros(out_shape)\n        for batch in range(n):\n            f_row = self.kernel_size[0]\n            f_col = self.kernel_size[1]\n            for i in range(c):\n                current_img = input[batch,i]\n                for j in range(h):\n                    for k in range(w):\n                            output[batch, i, j, k] = np.max(current_img[j*self.stride: j*self.stride+f_row, k*self.stride: k*self.stride+f_col])             \n        self.size = output.shape\n        return output\n    \n    def __str__(self):\n        return self.type\n        \n        \n    def get_padding(self, input, pad = 1, full = 0):\n        # img size: n_batch, n_channels, w,h\n        padd = []\n        for i in range(input.shape[0]):\n            arr = np.full((input.shape[1],input.shape[2]+2*pad,input.shape[3]+2*pad),full)\n            arr[:, pad:-pad,pad:-pad] = input[i]\n            padd.append(arr)\n        return np.array(padd)\n    \n    def backward(self, prev_grad):\n        if prev_grad.shape != self.size:\n            prev_grad = np.reshape(self.size)\n        N, C, H, W =prev_grad.shape\n        dx = np.zeros_like(self.input)\n        \n        for n in range(N):\n            for c in range(C):\n                for i in range(H):\n                    for j in range(W):\n                        i_t, j_t = np.where(np.max(self.input[n, c, i * self.stride : i * self.stride + self.kernel_size[0], j * self.stride : j * self.stride + self.kernel_size[1]]) == self.input[n, c, i * self.stride : i * self.stride + self.kernel_size[0], j * self.stride : j * self.stride + self.kernel_size[1]])\n                        i_t, j_t = i_t[0], j_t[0]\n                        dx[n, c, i * self.stride : i * self.stride + self.kernel_size[0], j * self.stride : j * self.stride + self.kernel_size[1]][i_t, j_t] = prev_grad[n, c, i, j]\n        return dx","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.683050Z","iopub.execute_input":"2022-12-29T14:47:17.683412Z","iopub.status.idle":"2022-12-29T14:47:17.702001Z","shell.execute_reply.started":"2022-12-29T14:47:17.683381Z","shell.execute_reply":"2022-12-29T14:47:17.700767Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class linear():\n    def __init__(self, in_features: int, out_features: int, bias = True):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = np.random.rand(self.out_features, self.in_features)\n        self.type = 'linear'\n        \n        if bias:\n            self.bias = np.random.rand(self.out_features)\n        else:\n            self.bias = None\n            \n            \n    def __call__(self, input):\n        self.input = input\n        result = []\n        \n        for i in range(input.shape[0]):\n            summ = np.matmul(self.weight, input[i]) + self.bias\n            result.append(summ)\n        return np.array(result)\n    \n    def __str__(self):\n        return self.type\n    \n    def get_params(self):\n        return [self.weight, self.bias]\n    \n    def update_weights(self, new_params):\n        self.weight, self.bias = new_params\n    \n    def backward(self,prev_grad):\n        dw = np.dot(prev_grad.T, self.input)\n        db = prev_grad.mean(axis=0, keepdims=True)\n        db = db.reshape(db.shape[1])\n        dx = np.dot(self.weight.T, prev_grad.T).T\n        assert (dx.shape == self.input.shape)\n        assert (dw.shape == self.weight.shape)\n        assert (db.shape == self.bias.shape)\n        return dx, dw, db\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.703414Z","iopub.execute_input":"2022-12-29T14:47:17.703800Z","iopub.status.idle":"2022-12-29T14:47:17.717927Z","shell.execute_reply.started":"2022-12-29T14:47:17.703761Z","shell.execute_reply":"2022-12-29T14:47:17.716723Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class relu():\n    def __init__(self, inplace = False):\n        self.inplace = inplace\n        self.type = 'relu'\n        \n        \n    def __call__(self, input):\n        self.input = input\n        result = []\n        for i in range(input.shape[0]):\n            result.append(np.maximum(0, input[i]))\n        return np.array(result)\n    \n    \n    def __str__(self):\n        return self.type\n    \n    \n    def backward(self, grad):\n        grad = grad.reshape(self.input.shape)\n        return np.where(self.input>0,grad,0)\n\n\nclass softmax():\n    def __init__(self, dim):\n        self.dim = dim\n        self.type = 'softmax'\n    \n    def __str__(self):\n        return self.type\n    \n    def __call__(self, input):\n        y = np.exp(input - np.max(input, axis=self.dim, keepdims=True))\n        return y / np.sum(y, axis=self.dim, keepdims=True)\n    \n    def backward(self, prev_grad):\n        return prev_grad\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.721886Z","iopub.execute_input":"2022-12-29T14:47:17.722232Z","iopub.status.idle":"2022-12-29T14:47:17.732587Z","shell.execute_reply.started":"2022-12-29T14:47:17.722203Z","shell.execute_reply":"2022-12-29T14:47:17.731783Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def Flatten(input):\n    new = []\n    for i in range(input.shape[0]):\n        new.append(input[i].flatten())\n    return np.array(new)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.733964Z","iopub.execute_input":"2022-12-29T14:47:17.734260Z","iopub.status.idle":"2022-12-29T14:47:17.748013Z","shell.execute_reply.started":"2022-12-29T14:47:17.734233Z","shell.execute_reply":"2022-12-29T14:47:17.747082Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# CrossEntropyLoss\nclass CELoss():\n    def __init__(self):\n        pass\n    \n    \n    def __call__(self, y_pred, y_true):\n         true_class_logits = y_pred[np.arange(len(y_pred)), y_true]\n    \n         cross_entropy = - true_class_logits + np.log(np.sum(np.exp(y_pred), axis=-1))\n         return np.mean(cross_entropy)\n        \n        \n    def backward(self, y_pred, y_true):\n        ones_true_class = np.zeros_like(y_pred)\n        ones_true_class[np.arange(len(y_pred)),y_true] = 1\n        softmax = np.exp(y_pred) / np.exp(y_pred).sum(axis=-1,keepdims=True)\n    \n        return (-ones_true_class + softmax) / y_pred.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.749277Z","iopub.execute_input":"2022-12-29T14:47:17.749581Z","iopub.status.idle":"2022-12-29T14:47:17.760584Z","shell.execute_reply.started":"2022-12-29T14:47:17.749554Z","shell.execute_reply":"2022-12-29T14:47:17.759270Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Adam():\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        self.params = np.array(params)\n        self.lr = lr\n        self.beta1, self.beta2 = betas\n        self.eps = eps\n        self.weight_decay = weight_decay\n        \n        self.steps = np.zeros(len(self.params))\n        self.exp_avg = np.zeros_like(self.params)\n        self.exp_avg_sq = np.zeros_like(self.params)\n        \n    def __call__(self,):\n        pass\n    \n    def step(self, grads):\n        for i in range(len(self.params)):\n            grad = grads[i]  # получаем градиенты слоя модели\n            \n            if self.steps[i] == 0:\n                # Momentum \n                self.exp_avg[i] = np.zeros_like(self.params[i])\n                # RMS Prop componenet. \n                self.exp_avg_sq[i] = np.zeros_like(self.params[i])\n                    \n            self.steps[i] += 1\n            if self.weight_decay != 0:\n                    grad = grad.add(self.weight_decay, self.params[i])\n            \n            # Momentum\n            self.exp_avg[i][0] = np.multiply(self.exp_avg[i][0], self.beta1) + (1 - self.beta1) * grad[0]\n            self.exp_avg[i][1] = np.multiply(self.exp_avg[i][1], self.beta1) + (1 - self.beta1) * grad[1]\n            # RMS\n            self.exp_avg_sq[i][0] = np.multiply(self.exp_avg_sq[i][0], self.beta2) + (1-self.beta2)*(grad[0]*grad[0])\n            self.exp_avg_sq[i][1] = np.multiply(self.exp_avg_sq[i][1], self.beta2) + (1-self.beta2)*(grad[1]*grad[1])\n            \n            self.exp_avg_sq[i][0] = self.exp_avg_sq[i][0].astype('float')\n            self.exp_avg_sq[i][1] = self.exp_avg_sq[i][1].astype('float')\n            \n            denom_w = np.sqrt(self.exp_avg_sq[i][0]) + self.eps\n            denom_b = np.sqrt(self.exp_avg_sq[i][1]) + self.eps\n\n            bias_correction1 = 1 / (1 - self.beta1 ** self.steps[i])\n            bias_correction2 = 1 / (1 - self.beta2 ** self.steps[i])\n                \n            adapted_learning_rate = self.lr * bias_correction1 / math.sqrt(bias_correction2)\n\n            self.params[i][0] = self.params[i][0] - adapted_learning_rate * self.exp_avg[i][0] / denom_w\n            self.params[i][1] = self.params[i][1] - adapted_learning_rate * self.exp_avg[i][1] / denom_b\n        return self.params       \n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.761998Z","iopub.execute_input":"2022-12-29T14:47:17.762344Z","iopub.status.idle":"2022-12-29T14:47:17.781313Z","shell.execute_reply.started":"2022-12-29T14:47:17.762313Z","shell.execute_reply":"2022-12-29T14:47:17.780274Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class model():\n    def __init__(self, num_classes = 10):\n        self.conv1 = conv2d(in_channels = 1, out_channels = 6, kernel_size = (5,5), padding = 2)\n        self.relu1 = relu()\n        self.maxpool1 = maxpool2d(kernel_size=(2, 2), stride=2)\n        \n        self.conv2 = conv2d(in_channels = 6, out_channels = 16, kernel_size = (5,5))\n        self.relu2 = relu()\n        self.maxpool2 = maxpool2d(kernel_size=(2, 2), stride=2)\n        \n        self.fc1 = linear(in_features=5*5*16, out_features=120)\n        self.relu3 = relu()\n        self.fc2 = linear(in_features=120, out_features=84)\n        self.relu4 = relu()\n        self.fc3 = linear(in_features=84, out_features=10)\n        self.softmax = softmax(dim=1)\n        \n        self.layers_list = [self.conv1,self.relu1, self.maxpool1, \n                            self.conv2, self.maxpool2, self.relu2, \n                            self.fc1, self.relu3, self.fc2, self.relu4, self.fc3, self.softmax]\n    \n    def __call__(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.maxpool1(x)\n        x = self.conv2(x)\n        x = self.maxpool2(x)\n        x = self.relu2(x)\n        x = Flatten(x)\n        x = self.fc1(x)\n        x = self.relu3(x)\n        x = self.fc2(x)\n        x = self.relu4(x)\n        x = self.fc3(x)\n        output = self.softmax(x)\n\n        return output\n\n    def get_backward(self, grad):\n        lst = self.layers_list.copy()\n        lst.reverse()\n        grads = []\n        for item in lst:\n#             print(item.__str__() )\n            if item.__str__() not in ['softmax', 'relu', 'maxpool']:\n                grad, dw,db = item.backward(grad)\n                grads.append([dw, db])\n            else:\n                grad = item.backward(grad)\n        grads.reverse()        \n        return grads\n                \n        \n    def update_weights(self, new_params):\n        i = 0\n        for item in self.layers_list:\n            if item.__str__() not in ['softmax', 'relu', 'maxpool']:\n                item.update_weights(new_params[i])\n                i+=1\n    \n    def get_params(self):\n        params = []\n        for item in self.layers_list:\n            if item.__str__() not in ['softmax', 'relu', 'maxpool']:\n                params.append(item.get_params())\n        return params","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.783528Z","iopub.execute_input":"2022-12-29T14:47:17.784066Z","iopub.status.idle":"2022-12-29T14:47:17.801992Z","shell.execute_reply.started":"2022-12-29T14:47:17.784023Z","shell.execute_reply":"2022-12-29T14:47:17.800813Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class train():\n    def __init__(self, model, data, n_epoch):\n        self.model = model\n        self.train_data, self.val_data = data\n        self.n_epoch = n_epoch\n        \n        self.criterion = CELoss()\n        self.opt = Adam(model.get_params() )\n    \n    def __call__(self):\n        history = []\n        log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss {v_loss:0.4f} \\\n                                         train_acc {t_acc:0.4f} val_acc {v_acc:0.4f} \\\n                                         train_f1 {t_f1:0.4f} val_f1 {v_f1:0.4f}\"\n        with tqdm(desc=\"epoch\", total=self.n_epoch) as pbar_outer:\n            for epoch in range(self.n_epoch):\n                train_loss, train_acc, train_f1 = self.fit()\n            \n                val_loss, val_acc, val_f1 = self.val()\n                \n                pbar_outer.update(1)\n                history.append((train_loss, train_acc, val_loss, val_acc, train_f1, val_f1))\n                tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,v_loss=val_loss, \n                                               t_acc=train_acc, v_acc=val_acc, \n                                               t_f1 = train_f1, v_f1 =val_f1))\n        return history\n\n    \n    def fit(self):\n            running_loss = 0.0\n            running_corrects = 0\n            processed_data = 0\n            train_f1_score = []\n            for inputs, labels in self.train_data:\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels) # считаем loss\n                print('loss', loss)\n                \n                grads = self.model.get_backward(self.criterion.backward(outputs, labels)) # считаем градиент loss функции и градиенты слоев модели\n                self.model.update_weights(self.opt.step(grads))  # считаем шаг для обновления весов и обновляем их\n                \n                preds = np.argmax(outputs, 1)\n                running_loss += loss * inputs.shape[0]\n                running_corrects += np.sum(preds == labels.data)\n                processed_data += inputs.shape[0]\n                train_f1_score.append(f1_score(labels, preds, average='macro'))\n              \n            train_loss = running_loss / processed_data\n            train_acc = running_corrects / processed_data\n            train_f1 = np.average(train_f1_score)\n            return train_loss, train_acc, train_f1\n    \n    def val(self):\n            running_loss = 0.0\n            running_corrects = 0\n            processed_size = 0\n            val_f1_score = []\n        \n            for inputs, labels in self.val_data:\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                print('val_loss', loss)\n                preds = np.argmax(outputs, 1)\n\n                running_loss += loss * inputs.shape[0]\n                running_corrects += np.sum(preds == labels)\n                processed_size += inputs.shape[0]\n                val_f1_score.append(f1_score(labels, preds, average='macro'))\n            val_loss = running_loss / processed_size\n            val_acc = running_corrects / processed_size\n            val_f1 = np.average(val_f1_score)\n            return val_loss, val_acc, val_f1","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.803417Z","iopub.execute_input":"2022-12-29T14:47:17.803960Z","iopub.status.idle":"2022-12-29T14:47:17.820108Z","shell.execute_reply.started":"2022-12-29T14:47:17.803928Z","shell.execute_reply":"2022-12-29T14:47:17.819269Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"datas1 = make_data(MNIST_train.train_data[:500], MNIST_train.train_labels[:500])\ntrain_dataset = custom_loader(datas1, 100)\ndatas2 = make_data(MNIST_train.test_data[:100], MNIST_train.test_labels[:100])\nval_dataset = custom_loader(datas2, 10)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.821386Z","iopub.execute_input":"2022-12-29T14:47:17.822326Z","iopub.status.idle":"2022-12-29T14:47:17.856202Z","shell.execute_reply.started":"2022-12-29T14:47:17.822283Z","shell.execute_reply":"2022-12-29T14:47:17.855156Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n  warnings.warn(\"test_data has been renamed data\")\n/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n  warnings.warn(\"test_labels has been renamed targets\")\n","output_type":"stream"}]},{"cell_type":"code","source":"lenet = model()\nstart_train = train(lenet, (train_dataset, val_dataset), n_epoch = 5)\nhis = start_train()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T14:47:17.857598Z","iopub.execute_input":"2022-12-29T14:47:17.857957Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  This is separate from the ipykernel package so we can avoid doing imports until\nepoch:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"loss 2.4611501717344746\nloss 2.4011501717344745\nloss 2.3711501717344747\nloss 2.3811501717344745\nloss 2.3611501717344745\nval_loss 2.4611501717344746\nval_loss 2.3611501717344745\nval_loss 2.3611501717344745\nval_loss 2.4611501717344746\nval_loss 2.2611501717344744\nval_loss 2.2611501717344744\nval_loss 2.4611501717344746\nval_loss 2.3611501717344745\nval_loss 2.3611501717344745\n","output_type":"stream"},{"name":"stderr","text":"epoch:  20%|██        | 1/5 [26:22<1:45:31, 1582.85s/it]","output_type":"stream"},{"name":"stdout","text":"val_loss 2.4611501717344746\n\nEpoch 001 train_loss: 2.3952                                          val_loss 2.3812                                          train_acc 0.0660                                          val_acc 0.0800                                          train_f1 0.0122                                          val_f1 0.0203\nloss 2.4611501717344746\nloss 2.4011501717344745\nloss 2.3711501717344747\nloss 2.3811501717344745\nloss 2.3611501717344745\n","output_type":"stream"}]},{"cell_type":"code","source":"loss, acc, val_loss, val_acc, tr_f1, val_f1= zip(*his)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(15, 25))\naxs[0, 0].plot(loss, label=\"train\")\naxs[0, 0].plot(val_loss, label=\"val\")\naxs[0, 0].set_title('Loss')\naxs[0, 0].set_xlabel(\"epochs\")\naxs[0, 0].set_ylabel(\"loss\")\naxs[0, 0].legend()\naxs[0, 1].plot(acc, label=\"train\")\naxs[0, 1].plot(val_acc, label=\"val\")\naxs[0, 1].set_title('Accuracy')\naxs[0, 1].set_xlabel(\"epochs\")\naxs[0, 1].set_ylabel(\"loss\")\naxs[0, 1].legend()\naxs[0, 2].plot(tr_f1, label=\"ADAM\")\naxs[0, 2].plot(val_f1, label=\"ADASMOOTH\")\naxs[0, 2].set_title('f1-score')\naxs[0, 2].set_xlabel(\"epochs\")\naxs[0, 2].set_ylabel('accuracy')\naxs[0, 2].legend()\nplt.savefig('gc.jpg')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}