{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install ipymarkup","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport torch\nfrom torch import nn\nimport re\nfrom tqdm import tqdm, tqdm_notebook\nfrom torch.optim import Adam, AdamW\nimport torchtext\nfrom torchtext.data import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\npd.options.mode.chained_assignment = None  # default='warn'\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom ipymarkup import show_span_box_markup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-16T17:22:16.233929Z","iopub.execute_input":"2023-01-16T17:22:16.234559Z","iopub.status.idle":"2023-01-16T17:22:16.270438Z","shell.execute_reply.started":"2023-01-16T17:22:16.234510Z","shell.execute_reply":"2023-01-16T17:22:16.268899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Для версии Андрея\n# PATH1 = '/kaggle/input/rurebustraindata/train_part_1/train_part_1'\n# PATH2 = '/kaggle/input/rurebustraindata/train_part_2/train_part_2'\n# PATH3 = '/kaggle/input/rurebustraindata/train_part_3/train_part_3'\n# Для версии Гарри и Тани\nPATH1 = '/kaggle/input/rurebus-train-data/train_part_1/train_part_1'\nPATH2 = '/kaggle/input/rurebus-train-data/train_part_2/train_part_2'\nPATH3 = '/kaggle/input/rurebus-train-data/train_part_3/train_part_3'\nPATH4 = '/kaggle/input/rurebus-train-data/test_ner_only' #чтобы все ок было надо обновить датасет до последней версии\n# имена сущностей\nNAMES = ['BIN', 'SOC', 'MET', 'CMP', 'ECO', 'INST', 'ACT', 'QUA']\n# тэги для разметки\nTAG = ['B-bin', 'I-bin', 'B-soc','I-soc','B-met', 'I-met','B-cmp','I-cmp','B-eco', 'I-eco','B-inst','I-inst','B-act','I-act','B-qua','I-qua']","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:21:57.377434Z","iopub.execute_input":"2023-01-16T15:21:57.378377Z","iopub.status.idle":"2023-01-16T15:21:57.385838Z","shell.execute_reply.started":"2023-01-16T15:21:57.378336Z","shell.execute_reply":"2023-01-16T15:21:57.384386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В тесте присутствует множество тестов разбитых по частям (part_1, part_2 и тд.). Они представляют собой логически целое однако среди нет переносов в аннотациях. \nВ аннотации присутствует следующая информация: именование сущности, ее класс а также местоположение в тексте (посимвольное).\nЧто касается отношений то дается информация о номере сущности и классе связи.\nДля дальнейшем работы необходимо понять какой формат входных данных нужен для cnn lstm \n","metadata":{}},{"cell_type":"code","source":"# подготовка данных для обучения\ndef make_data(lst,path):        \n    res_df = pd.DataFrame(columns = ['word', 'tag'])\n    with tqdm(desc=\"n\", total=len(lst)) as pbar_outer:\n        for item in lst:\n            df1 = make_ann(item,path)\n            df2 = make_text(item, df1,path)\n            res_df = pd.concat([res_df, df2], ignore_index=True)\n            pbar_outer.update(1)\n    return res_df\n\n\n# продготовка аннотаций\ndef make_ann(file,path):\n    # открываю файл\n    ann_df = pd.read_csv(path+'/'+file+'.ann', sep='\\t', engine='python', header=None, on_bad_lines='skip') # здесь добавлено скипанье плохих строк(если например столбцов 6, а в трок только 4), наверно это не оч хорошо надо что-то придумать\n    # нормальные названия для столбцов\n    ann_df.rename(columns = {1:'class', 2:'words'}, inplace = True )\n    # разделяю в разные столбцы классы и координаты\n    ann_df.insert(2, 'coords' , ann_df['class'])\n    ann_df['class'] = ann_df['class'].apply(lambda x: x.split(\" \")[0])\n    ann_df['coords'] = ann_df['coords'].apply(lambda x: x.split(\" \")[1:])\n    # удаляю нафиг строки с отношениями\n    ann_df = ann_df.dropna()\n    ann_df.reset_index(drop= True , inplace= True )\n    # для удобства\n    ann_df.insert(2, 'coords1' , ann_df['coords'])\n    ann_df['coords1'] = ann_df['coords1'].apply(lambda x: int(x[0]))\n    ann_df['words'] = ann_df['words'].apply(lambda x: my_split(x.split(\" \")))\n    ann_df['words'] = ann_df['words'].apply(lambda x: del_all(x))\n    ann_df['words'] = ann_df['words'].apply(lambda x: [item.strip() for item in x if item not in ['','»', '«',':',' ']])\n    ann_df = ann_df.sort_values(by='coords1')\n    ann_df.reset_index(drop= True , inplace= True )\n    return ann_df\n\n\n# разметка\ndef make_text(file, df,path):\n    # открываем файл и записываем его в dataframe\n    with open(path+'/'+file+'.txt') as f:\n        lines = f.readlines()\n    text_df = pd.DataFrame({'word':lines})\n    # считаем длины строк для удобства дальнейшей разметки\n    text_df.insert(1, 'len' , text_df['word'].copy())\n    text_df['len'] = text_df['len'].apply(lambda x: len(x)) # считаем длину строки\n    new_lens = [text_df['len'][0]] # считаем длину предыдущих строк + длина новой строки\n    for i in range(1, len(list(text_df['len']))):\n        new_lens.append(sum(list(text_df['len'])[:i+1]))\n    text_df.insert(2, 'new_len' , new_lens)\n    # добавляем столбец для разметки\n    text_df.insert(3, 'tag', 0)\n    # удаляем \\n\n    text_df['word'] = text_df['word'].apply(lambda x: re.split('\\n',x)[0])\n    #удаляем строрки с []\n    idx = [i for i in range(len(text_df)) if len(text_df['word'][i])==0]\n    text_df = text_df.drop(index=idx)\n    text_df.reset_index(drop = True, inplace= True)\n    # делаем разметку\n    # находим индекс строки для каждой аннотации\n    df = find_rows(text_df, df)\n#     print(df.head(5))\n    # преобразовываем строку в массив\n    text_df['word'] = text_df['word'].apply(lambda x: my_split([item for item in re.split(' ',x) if item != '']))\n    text_df['word'] = text_df['word'].apply(lambda x: del_all(x))\n    text_df['word'] = text_df['word'].apply(lambda x: [item.strip() for item in x if item not in ['',' ']])\n    # момент разметки\n    text_df = make_markup(text_df, df)\n    text_df = last_changes(text_df)\n#     # удаляем лишнее\n    del text_df['len']\n    del text_df['new_len']\n    text_df['word'] = text_df['word'].apply(lambda x: [item.lower() for item in x if item not in ['',' ']])\n    idx = [i for i in range(len(text_df)) if len(text_df['word'][i])==0]\n    text_df = text_df.drop(index=idx)\n    text_df.reset_index(drop = True, inplace= True)\n    return text_df\n\n\n# отделяем все что можно\ndef my_split(lst):\n    for k in range(3):\n        for i in range(len(lst)-1,-1,-1):\n            if lst[i] in ['',' ']:\n                lst.pop(i)\n            else:\n                lst[i] = lst[i].replace('\\xa0', ' ')\n                lst[i] = lst[i].replace('\\t', ' ')\n                lst[i]=lst[i].replace('………………','')\n                lst[i]=lst[i].replace('………','')\n                lst[i] = lst[i].replace('……','')\n                idx = []\n                lens = len(lst[i])\n                for item in ['+', ')', '»',';','.',',', '\"','(', '«',':',' ', '-\\t','\\\\', '/','”','“','-','–','_________','*','№','%']:\n                     idx.append(lst[i].find(item))\n                for item in ['\\xa0','\"','.']:\n                    if lst[i].endswith(item) and lens-1 not in idx:\n                        idx.append(lens-1)                    \n                idx.sort(reverse=True)\n                for item in idx:\n                    if item!=-1 and item!=0:\n                        lst.insert(i+1, lst[i][item])\n                        lst.insert(i+2, lst[i][item+1:])\n                        lst[i] = lst[i][:item]\n                    elif item!=-1 and item == 0:\n                        lst.insert(i+1, lst[i][item+1:])\n                        lst[i] = lst[i][item]\n                    elif item!= -1 and item == lens-1:\n                        lst.insert(i+1, lst[i][item])\n                        lst[i] = lst[i][:item] \n    return lst\n\n\ndef del_all(lst):\n    for i in range(len(lst)-1,-1,-1):\n        split = lst[i]\n        split = split.split(' ')\n        if len(split) >1:\n            for j in range(len(split)):\n                lst.insert(i+1+j, split[j])\n            lst.pop(i)\n    return lst\n\n\n# обработка строк без сущностей и оставшихся слов\ndef last_changes(df):\n    for i in range(len(df)):\n        if df['tag'][i] == 0:\n            df['tag'][i] = ['O'] * len(df['word'][i])\n        else:\n#             df['tag'][i] = my_split(df['tag'][i])\n            llst = []\n            for item in df['tag'][i]:\n                if item in TAG:\n                    llst.append(item)\n                else:\n                    llst.append('O')\n            df['tag'][i] = llst\n        assert (len(df['word'][i]) == len(df['tag'][i]))\n    return df\n\n# находим индекс строки для каждой аннотации\ndef find_rows(txt_df, ann_df):\n    ann_df.insert(4, 'idx' , 0)\n    for i in range(len(ann_df)):\n        for j in range(len(txt_df)):\n            start = txt_df['new_len'][j]-txt_df['len'][j]\n            end = txt_df['new_len'][j]\n            if int(ann_df['coords'][i][0]) in np.arange(start, end):\n                ann_df['idx'][i] = j\n                break\n            else:\n                pass\n    return ann_df\n    \n# основная часть разметки\ndef make_markup(text_df, ann_df):\n    for i in range(len(ann_df)):\n        words_count = len(ann_df['words'][i])\n        lens = [len(item) for item in ann_df['words'][i]] \n        row = text_df.iloc[[ann_df['idx'][i]]]\n#         print(row['word'].item())\n        if row['tag'].item() == 0:\n                        text_df['tag'][ann_df['idx'][i]] = text_df['word'][ann_df['idx'][i]].copy()\n                        row['tag'] = row['word']\n        for k in range(words_count):\n                if k == 0:\n                    idx = (row['tag'].item()).index(ann_df['words'][i][k])\n                    text_df['tag'][ann_df['idx'][i]][idx] = 'B-'+ ann_df['class'][i].lower()\n                else:\n                    idx = row['tag'].item().index(ann_df['words'][i][k])\n                    text_df['tag'][ann_df['idx'][i]][idx] = 'I-'+ ann_df['class'][i].lower()\n    return text_df","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:49:53.260046Z","iopub.execute_input":"2023-01-16T15:49:53.261207Z","iopub.status.idle":"2023-01-16T15:49:53.316094Z","shell.execute_reply.started":"2023-01-16T15:49:53.261161Z","shell.execute_reply":"2023-01-16T15:49:53.314385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# список всех файлов для обучения\ndata_lst = list(set([item[:-4] for item in os.listdir(PATH1)]))\ndata_lst.remove('.stats_c')\ntrain_df = make_data(data_lst, PATH1)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:26:36.128452Z","iopub.execute_input":"2023-01-16T15:26:36.129276Z","iopub.status.idle":"2023-01-16T15:27:44.455780Z","shell.execute_reply.started":"2023-01-16T15:26:36.129219Z","shell.execute_reply":"2023-01-16T15:27:44.454352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#файлы для валидации\ndata_lst = list(set([item[:-4] for item in os.listdir(PATH2)]))\ndata_lst = data_lst[0]\nval_df = make_data([data_lst], PATH2)\nval_df","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:26:34.405469Z","iopub.execute_input":"2023-01-16T15:26:34.409071Z","iopub.status.idle":"2023-01-16T15:26:35.219780Z","shell.execute_reply.started":"2023-01-16T15:26:34.408999Z","shell.execute_reply":"2023-01-16T15:26:35.218277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab_from_iterator(train_df['word'], min_freq=1, specials=[\"<UNK>\"])\nvocab.set_default_index(vocab[\"<UNK>\"])\nvocab_lables = build_vocab_from_iterator(train_df['tag'], min_freq=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:23:08.291664Z","iopub.execute_input":"2023-01-16T15:23:08.292570Z","iopub.status.idle":"2023-01-16T15:23:08.531146Z","shell.execute_reply.started":"2023-01-16T15:23:08.292509Z","shell.execute_reply":"2023-01-16T15:23:08.530078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab(train_df['word'][0])","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:23:08.704453Z","iopub.execute_input":"2023-01-16T15:23:08.705294Z","iopub.status.idle":"2023-01-16T15:23:08.714788Z","shell.execute_reply.started":"2023-01-16T15:23:08.705223Z","shell.execute_reply":"2023-01-16T15:23:08.713202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_tl(df):\n    tokens = []\n    lables = []\n    max_len = 0\n    for ind in df.index:\n        tokens.append(vocab(df['word'][ind]))\n        lables.append(vocab_lables(df['tag'][ind]))\n        if len(df['word'][ind]) > max_len:\n            max_len = len(df['word'][ind])\n    df['tokens'] = tokens\n    df['lables'] = lables\n    return df,max_len\n\n\ndef make_pad(df):\n    list_sent = []\n    list_labels = []\n    for ind in df.index:\n        list_sent.append(df['tokens'][ind])\n        list_labels.append(df['lables'][ind])\n    padded_sent = pad_sequences(list_sent)\n    padded_labels = pad_sequences(list_labels)\n    print(padded_sent.shape)\n    padd_df = pd.DataFrame(columns = ['sentence', 'labels'])\n    padd_df['sentence'] = pd.Series(padded_sent.tolist())\n    padd_df['labels'] = pd.Series(padded_labels.tolist())\n    return padd_df","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:23:08.717157Z","iopub.execute_input":"2023-01-16T15:23:08.718149Z","iopub.status.idle":"2023-01-16T15:23:08.728978Z","shell.execute_reply.started":"2023-01-16T15:23:08.718090Z","shell.execute_reply":"2023-01-16T15:23:08.727673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df,max_len1 = make_tl(train_df)\nval_df,max_len2 = make_tl(val_df)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:27:44.458385Z","iopub.execute_input":"2023-01-16T15:27:44.458823Z","iopub.status.idle":"2023-01-16T15:27:44.807094Z","shell.execute_reply.started":"2023-01-16T15:27:44.458775Z","shell.execute_reply":"2023-01-16T15:27:44.805167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(max_len1, ' ',max_len2)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:29:47.695685Z","iopub.execute_input":"2023-01-16T15:29:47.696143Z","iopub.status.idle":"2023-01-16T15:29:47.704649Z","shell.execute_reply.started":"2023-01-16T15:29:47.696107Z","shell.execute_reply":"2023-01-16T15:29:47.702699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# делаем паддинг \ntrain_df = make_pad(train_df)\nval_df = make_pad(val_df)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:30:03.565537Z","iopub.execute_input":"2023-01-16T15:30:03.566021Z","iopub.status.idle":"2023-01-16T15:30:03.918958Z","shell.execute_reply.started":"2023-01-16T15:30:03.565981Z","shell.execute_reply":"2023-01-16T15:30:03.917287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_df['sentence'][4]),len(val_df['sentence'][4]))","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:32:28.177901Z","iopub.execute_input":"2023-01-16T15:32:28.178483Z","iopub.status.idle":"2023-01-16T15:32:28.187749Z","shell.execute_reply.started":"2023-01-16T15:32:28.178438Z","shell.execute_reply":"2023-01-16T15:32:28.185775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(x: np.ndarray, vocab_len: int) -> np.ndarray:\n    \"\"\"\n    Args:\n        x - одномерный массив значений словаря\n        vocab_len - длина словаря\n    Выход:\n        двумерный массив encoded, где encoded[i] - результат one hot кодирования x[i]\n    \"\"\"\n    encoded = np.zeros((len(x), vocab_len))\n    for i in range(len(x)):\n        encoded[i][x[i]] = 1\n    return encoded\n\ntext_vocab_len = len(vocab)\ntarget_vocab_len = len(vocab_lables)\n\nclass TokenDataset(torch.utils.data.Dataset):\n    def __init__(self, data, text_vocab_len = text_vocab_len, target_vocab_len = target_vocab_len, classes = None,\n                 transform=None, target_transform=None):\n        self.data = data\n        self.sequence_len = len(data.iloc[0][0])\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        tokens, tag = self.data.iloc[idx]\n        return torch.Tensor(tokens).int(), torch.Tensor(one_hot(tag, target_vocab_len))\n        # return torch.Tensor(tokens).int(), torch.Tensor(tag)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:32:33.195060Z","iopub.execute_input":"2023-01-16T15:32:33.195587Z","iopub.status.idle":"2023-01-16T15:32:33.210122Z","shell.execute_reply.started":"2023-01-16T15:32:33.195545Z","shell.execute_reply":"2023-01-16T15:32:33.207657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = {\n    'train': TokenDataset(train_df),\n    'val': TokenDataset(val_df)\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:32:56.112552Z","iopub.execute_input":"2023-01-16T15:32:56.113058Z","iopub.status.idle":"2023-01-16T15:32:56.120220Z","shell.execute_reply.started":"2023-01-16T15:32:56.113018Z","shell.execute_reply":"2023-01-16T15:32:56.118828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = {\n    'train':\n    torch.utils.data.DataLoader(datasets['train'],\n                                batch_size=16,\n                                shuffle=True,\n                                num_workers=0),  # for Kaggle\n    'val':\n    torch.utils.data.DataLoader(datasets['val'],\n                                batch_size=16,\n                                shuffle=False,\n                                num_workers=0)  # for Kaggle\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:23:09.949037Z","iopub.execute_input":"2023-01-16T15:23:09.950549Z","iopub.status.idle":"2023-01-16T15:23:09.959972Z","shell.execute_reply.started":"2023-01-16T15:23:09.950478Z","shell.execute_reply":"2023-01-16T15:23:09.958683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:23:09.961875Z","iopub.execute_input":"2023-01-16T15:23:09.962313Z","iopub.status.idle":"2023-01-16T15:23:09.973909Z","shell.execute_reply.started":"2023-01-16T15:23:09.962241Z","shell.execute_reply":"2023-01-16T15:23:09.972580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_LSTM(nn.Module):\n    def __init__(self, vocab_size, n_classes, embedding_dim=250, hidden_size = 32, filters=((2, 10), (3, 8))):\n        super().__init__()\n        \n        self.embeddings_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=200, kernel_size=3,padding=1)\n        self.pool1 = nn.MaxPool1d(2)\n        input_size = 100\n        self.hidden_size = hidden_size\n        self.lstm_layer = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(self.hidden_size, n_classes)\n        outputs = []\n\n    def forward(self, inputs):\n        projections = self.embeddings_layer.forward(inputs) \n        projections = projections.transpose(1, 2)\n        projections = self.conv1(projections)\n        projections = projections.transpose(1, 2)\n        projections = self.pool1(projections)\n        output, (final_hidden_state, final_cell_state) = self.lstm_layer(projections)\n        output = output.reshape(-1, self.hidden_size)\n        output = self.fc(output)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:23:09.975894Z","iopub.execute_input":"2023-01-16T15:23:09.976319Z","iopub.status.idle":"2023-01-16T15:23:09.988600Z","shell.execute_reply.started":"2023-01-16T15:23:09.976283Z","shell.execute_reply":"2023-01-16T15:23:09.987234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, num_epochs=3):\n    all_true_labels = []\n    all_preds = []\n    inputs_str = []\n    loss_list = {'train' : [], 'val':[]}\n    acc_list = {'train' : [], 'val':[]}\n    f1_list = {'train' : [], 'val':[]}\n#     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n#                                                    step_size=10,\n#                                                    gamma=0.2)\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                print('start train')\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            all_true_labels = []\n            all_preds = []\n            inputs_str = []\n            for inputs, labels in tqdm(dataloader[phase]):\n                batch_size, n_words, n_classes = labels.shape\n                labels = labels.reshape(-1, n_classes).to(device)\n                outputs = model(inputs.to(device))\n                \n                loss = criterion(outputs, labels)\n                if phase == 'train':\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                _, preds = torch.max(outputs, 1)\n                running_loss += loss.item() #* inputs.size(0)\n                _, labels = torch.max(labels, 1)\n\n                all_true_labels.extend(labels.tolist())\n                all_preds.extend(preds.tolist())\n            epoch_loss = running_loss /len(dataloader[phase])\n            epoch_acc = accuracy_score(all_preds, all_true_labels)\n            epoch_f1 = f1_score(all_preds, all_true_labels, average='macro')\n#             lr_scheduler.step()  \n            print('{} loss: {:.4f}, acc: {:.4f}, f1: {:.4f}'.format(phase,\n                                                        epoch_loss,\n                                                        epoch_acc,\n                                                        epoch_f1            \n                                                        ))\n            loss_list[phase].append(epoch_loss)\n            acc_list[phase].append(epoch_acc.tolist())\n            f1_list[phase].append(epoch_f1.tolist())\n    return all_true_labels, all_preds, inputs_str, loss_list, acc_list, f1_list","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:07:58.589945Z","iopub.execute_input":"2023-01-16T19:07:58.590442Z","iopub.status.idle":"2023-01-16T19:07:58.617656Z","shell.execute_reply.started":"2023-01-16T19:07:58.590404Z","shell.execute_reply":"2023-01-16T19:07:58.616433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = len(vocab_lables)\nmodel = CNN_LSTM(len(vocab), n_classes = n_classes).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = Adam(model.parameters(), lr = 3e-4)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:07:49.408197Z","iopub.execute_input":"2023-01-16T19:07:49.408680Z","iopub.status.idle":"2023-01-16T19:07:49.445283Z","shell.execute_reply.started":"2023-01-16T19:07:49.408645Z","shell.execute_reply":"2023-01-16T19:07:49.443788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epox_num = 15\nall_true_labels, all_preds, inputs_str, loss, acc, f1 = train_model(model, criterion, optimizer, epox_num)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:08:03.475832Z","iopub.execute_input":"2023-01-16T19:08:03.476910Z","iopub.status.idle":"2023-01-16T19:33:25.417247Z","shell.execute_reply.started":"2023-01-16T19:08:03.476860Z","shell.execute_reply":"2023-01-16T19:33:25.416030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epox_list = [i for i in range(epox_num)]\ndef graf(loss, acc):\n    \n    fig, ax = plt.subplots(2, 3, figsize=(26, 13))\n    ax[0, 0].plot(epox_list, loss['train'])\n    ax[0, 0].set_title(\"Изменение потерь на обучающей выборке\")\n    ax[0, 1].plot(epox_list, acc['train'])\n    ax[0, 1].set_title(\"Изменение точности на обучающей выборке\")\n    ax[0, 2].plot(epox_list, f1['train'])\n    ax[0, 2].set_title(\"Изменение f1-score на обучающей выборке\")\n    ax[1, 0].plot(epox_list, loss['val'])\n    ax[1, 0].set_title(\"Изменение потерь на валидационной выборке\")\n    ax[1, 1].plot(epox_list, acc['val'])\n    ax[1, 1].set_title(\"Изменение точности на валидационной выборке\")\n    ax[1, 2].plot(epox_list, f1['val'])\n    ax[1, 2].set_title(\"Изменение f1-score на валидационной выборке\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:33:38.982970Z","iopub.execute_input":"2023-01-16T19:33:38.984488Z","iopub.status.idle":"2023-01-16T19:33:38.995881Z","shell.execute_reply.started":"2023-01-16T19:33:38.984433Z","shell.execute_reply":"2023-01-16T19:33:38.994529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ngraf(loss, acc)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:33:41.039968Z","iopub.execute_input":"2023-01-16T19:33:41.041433Z","iopub.status.idle":"2023-01-16T19:33:42.711771Z","shell.execute_reply.started":"2023-01-16T19:33:41.041379Z","shell.execute_reply":"2023-01-16T19:33:42.710501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test(model,dataloader,dataset):\n    all_true_labels = []\n    all_preds = []\n    inputs_str = []\n    acc_list = []\n    model.eval()\n    for inputs, labels in tqdm(dataloader):\n        batch_size, n_words, n_classes = labels.shape\n        labels = labels.reshape(-1, n_classes).to(device)\n        outputs = model(inputs.to(device))\n        _, preds = torch.max(outputs, 1)\n        _, labels = torch.max(labels, 1)\n        all_true_labels.extend(labels.tolist())\n        all_preds.extend(preds.tolist())\n    epoch_acc = accuracy_score(all_preds, all_true_labels)\n    epoch_f1 = f1_score(all_preds, all_true_labels, average='macro')\n    print('acc: {:.4f}, f1: {:.4f}'.format(epoch_acc, epoch_f1))\n    acc_list.append(epoch_acc.tolist())\n    return all_true_labels,all_preds","metadata":{"execution":{"iopub.status.busy":"2023-01-16T18:04:36.146611Z","iopub.execute_input":"2023-01-16T18:04:36.147712Z","iopub.status.idle":"2023-01-16T18:04:36.157790Z","shell.execute_reply.started":"2023-01-16T18:04:36.147667Z","shell.execute_reply":"2023-01-16T18:04:36.155603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_lst = list(set([item[:-4] for item in os.listdir(PATH4)]))\ndata_lst.remove('31339221025603182330049_24_part_1')\ndata_lst.remove('31339131024502051716072_24_part_1')\ndata_lst.remove('31339251033301001216016_20_part_2')\ndata_lst.remove('31339011021101006981035_9_part_1')\ndata_lst.remove('31339011021100987258005_5_part_2')\ndata_lst.remove('31339011061672000026002_4_part_2')\ndata_lst.remove('31339011026200597103018_8_part_1')\ndata_lst.remove('31339201027002952877006_14_part_2')\ntest_df = make_data(data_lst[:100], PATH4)\ntest_df,max_len1 = make_tl(test_df)\ntest_df = make_pad(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:34:00.830096Z","iopub.execute_input":"2023-01-16T19:34:00.830559Z","iopub.status.idle":"2023-01-16T19:34:47.157522Z","shell.execute_reply.started":"2023-01-16T19:34:00.830520Z","shell.execute_reply":"2023-01-16T19:34:47.156172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset =  TokenDataset(test_df)\ntest_dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=False, num_workers=0)\nall_t,all_p = make_test(model,test_dataloader,dataset)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:35:22.426550Z","iopub.execute_input":"2023-01-16T19:35:22.427038Z","iopub.status.idle":"2023-01-16T19:37:09.273607Z","shell.execute_reply.started":"2023-01-16T19:35:22.426998Z","shell.execute_reply":"2023-01-16T19:37:09.272241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# убираем палдинги \ndata = {'sentence': test_df['sentence'], 'real_cat': np.array_split(all_t, len(test_df['sentence'])), 'pred_cat':np.array_split(all_p, len(test_df['sentence']))}\ndf_test_see = pd.DataFrame(data)\nfor ind in df_test_see.index:\n    sent = df_test_see['sentence'][ind]\n    try:\n        len_pad = sent.index(next(filter(lambda x: x!=0, sent)))\n    except StopIteration:\n        pass\n    sent = sent[len_pad:]\n    sent = vocab.lookup_tokens(sent)\n    df_test_see.at[ind, 'sentence'] = sent \n    sent = df_test_see['real_cat'][ind]\n    sent = sent[len_pad:]\n    sent = vocab_lables.lookup_tokens(sent)\n    df_test_see.at[ind, 'real_cat'] = sent \n    sent = df_test_see['pred_cat'][ind]\n    sent = sent[len_pad:]\n    sent = vocab_lables.lookup_tokens(sent)\n    df_test_see.at[ind, 'pred_cat'] = sent \ndf_test_see","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:37:10.410544Z","iopub.execute_input":"2023-01-16T19:37:10.411450Z","iopub.status.idle":"2023-01-16T19:37:11.394543Z","shell.execute_reply.started":"2023-01-16T19:37:10.411397Z","shell.execute_reply":"2023-01-16T19:37:11.393324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_show(row,lbl):\n    spans = []\n    coords = 0\n    c1 = 0\n    c2 = 0\n    tag = ''\n    text = ''\n    last = 0\n    for i in range(len(row)):\n        text += row[i]+' '\n        if lbl[i] in TAG:\n            if 'B-' in lbl[i]:\n                c1 = coords\n                c2 = coords + len(row[i])\n                tag = lbl[i][2:].upper()\n                coords+=1+len(row[i])\n            elif 'I-' in lbl[i]:\n                c2+=1+len(row[i])\n                coords+=1+len(row[i])\n        elif lbl[i] =='O':\n            coords += len(row[i])+1\n        if (c1,c2,tag) !=(0, 0, ''):\n            if last == c1:\n                spans = spans[:-1]\n            spans.append((c1,c2,tag))\n            last = c1\n    return text, spans","metadata":{"execution":{"iopub.status.busy":"2023-01-16T18:56:12.590999Z","iopub.execute_input":"2023-01-16T18:56:12.591715Z","iopub.status.idle":"2023-01-16T18:56:12.601658Z","shell.execute_reply.started":"2023-01-16T18:56:12.591675Z","shell.execute_reply":"2023-01-16T18:56:12.600353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_idx = 5971\nrow = df_test_see['sentence'][row_idx]\ntrue_lbl = df_test_see['real_cat'][row_idx]\npred_lbl = df_test_see['pred_cat'][row_idx]\ntext,spans1 = make_show(row,true_lbl)\n_,spans2 = make_show(row,pred_lbl)\nprint('true:')\nshow_span_box_markup(text, spans1)\nprint('pred:')\nshow_span_box_markup(text, spans2)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T19:37:17.101620Z","iopub.execute_input":"2023-01-16T19:37:17.102065Z","iopub.status.idle":"2023-01-16T19:37:17.115286Z","shell.execute_reply.started":"2023-01-16T19:37:17.102025Z","shell.execute_reply":"2023-01-16T19:37:17.114355Z"},"trusted":true},"execution_count":null,"outputs":[]}]}