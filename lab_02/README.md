## Отчет
В данной работе нам необходимо было на датасете youtube-videos-dataset использовать модель bert для классификации заголовком по 4 категориям. Также нам необходимо было исследовать качество обучения различных комбинаций слоев модели.
### Архитектура Bert
BERT — это модель для предварительного обучения, обученная на Books Corpus с 800 млн слов и английской Википедии с 2500 млн слов. Эта модель является двунаправленной нейронной сетью-кодировщиком что означает что она использует левый и правый контекст слов. Базовая модель берта состоит из 12 слоев.
![bert](https://user-images.githubusercontent.com/58116790/209441945-493c2f8c-e357-4983-9ee0-01af3cd25b1a.png)
![bert](https://user-images.githubusercontent.com/58116790/209441763-ade1dda7-3225-4cef-b947-659ed38cfd92.png)

### Ход работы
В рамках этой лабораторной работы мы будет выполнять тонкую настроку модели Bert то есть дообучать ее. Крайне важно провести предварительную очистку данных поскольку данная модель сильно зависима от того сколько мусора будет содержаться в предложениях. Чтобы выявить основные проблемы мы рассмотрели первые 500 строк датафрейма. Были выявлены следующие проблемы:

1. Описание может состоять из нескольких предложений которые отделены символом |. Это является проблемой поскольку модель Bert может принимать максимум 2 предложения
2. Присутствие в тексте эмодзи, смайлов и тд
3. Присутствие слов на других языках
4. Специальные символы
5. Цифры, даты и тд
6. Слова выпадающие из контекста (то что в скобках) 

Проблемы с пункта 2-6 мы решили с помощью регулярных выражений. Однако проблема 1 так просто не решается. У нас было несколько гипотез для ее решения:
1. Оставить как есть, удалить символы |
2. Оставить 2 предложений. Остальные удалить
3. Оставить 1 наибольшее предложение. Остальные удалить 

Мы протестировали эти варианты. Наилучшим оказался вариант 3 поэтому его и будем использовать в дальнейшем. Следуюим нашим шагом является дообучение bert. Для этого мы разбивали весь датасет на две части где 80% использовалось для дообучения и 20% для валидации. Мы делали это на различных комбинациях слоев: первой половие, второй половине, все, только последний линейные и тд.

### Результаты
Обучение всей модели: 

![graph](https://user-images.githubusercontent.com/58116790/209847702-c48f0e97-8591-49ce-a839-a9bd33111e22.png)

Обучение только линейного слоя:
![graph](https://user-images.githubusercontent.com/58116790/209847774-826dc766-127d-43a6-b45a-39bd8dcaeccc.png)

Обучение только четных слоев и последнего линейного:
![graph](https://user-images.githubusercontent.com/58116790/209847887-f073dc7c-0cc0-43be-b1d5-c2ba6ec1ad12.png)

Обучение только не четных слоев и последнего линейного:
![graph](https://user-images.githubusercontent.com/58116790/209847932-cfe2b465-bb19-4fc5-a316-164f87e002f4.png)

Обучение только первой половины слоев и последнего линейного:
![graph](https://user-images.githubusercontent.com/58116790/209848757-99f245cd-baae-468e-8804-1baff12636f2.png)

Обучение только второй половины слоев и последнего линейного:
![graph](https://user-images.githubusercontent.com/58116790/209848556-a87fd0ef-cd43-4d49-ac57-918fb2d9cc58.png)
