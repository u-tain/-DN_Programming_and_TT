## Отчет
В данной работе нам необходимо было на датасете youtube-videos-dataset использовать модель bert для классификации заголовком по 4 категориям. Также нам необходимо было исследовать качество обучения различных комбинаций слоев модели.
### Архитектура Bert
BERT — это модель для предварительного обучения, обученная на Books Corpus с 800 млн слов и английской Википедии с 2500 млн слов. Эта модель является двунаправленной нейронной сетью-кодировщиком что означает что она использует левый и правый контекст слов. Базовая модель берта состоит из 12 слоев.
https://user-images.githubusercontent.com/58116790/209441945-493c2f8c-e357-4983-9ee0-01af3cd25b1a.png
